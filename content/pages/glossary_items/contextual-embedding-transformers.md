<!DOCTYPE html><html lang="en"><head><title="Contextual Embedding/Transformers"></head>
<body><p><font face="Poppins, Calibri, sans-serif" size="3"><b>Contextual Embedding/<br>Transformers</b></font></p>
<p><font face="Poppins, Calibri, sans-serif" size="2"><b>Chaya Liebeskind</b>, <i>Jerusalem College of Technology, Israel</i><br><a href="mailto:liebchaya@gmail.com" target="blank">liebchaya@gmail.com</a></font></p>
<p><font face="Poppins, Calibri, sans-serif" size="2"><b>Barbara Lewandowska-Tomaszczyk</b>, <i>University of Applied Sciences in Konin, Poland</i><br><a href="mailto:barbara.lewandowska-tomaszczyk@konin.edu.pl" target="blank">barbara.lewandowska-tomaszczyk@konin.edu.pl</a></font></p>
<p><font face="Poppins, Calibri, sans-serif" size="2"><br><br><br>Contextual embeddings, as opposed to context-free embeddings, consider the positioning of a word within a specific sentence or document. Transformers, a type of deep learning model, emerged as powerful tools in the creation of contextual embeddings. By employing the attention mechanism, transformers enable the model to assess the significance of distinct terms within a given sequence, thereby capturing complex contextual information. According to Devlin et al., the ability to comprehend context allows transformers, including BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), to generate representations of words that are more comprehensive and subtle, taking into account their diverse meanings in various contexts (for example, the same sentence: ‘The king smiled proudly.’) A transformer would examine not only the co-occurrence of words, but also their order and relationships within the sentence. It could be inferred that ‘smiled proudly’ modifies ‘king,’ implying a positive, triumphant smile, putting ‘king’ in a different vector location than when used in a different context, such as ‘The king ruled with an iron fist.’ Transformers have made substantial strides in the field of natural language processing (NLP) by enabling models to comprehend intricate linguistic connections and interdependencies. As a result, they have proven to be exceptionally efficient in a vast array of applications, including question answering and text summarisation.<br><br><br><br></font></p>
<p><font face="Poppins, Calibri, sans-serif" size="2"><b>Keywords</b>: </font></font></span></font><font color="#000000"><span style="text-decoration: none"><font face="calibri, sans-serif"><font size="2" style="font-size: 10pt">a</font></font></span></font><font color="#000000"><span style="text-decoration: none"><font face="calibri, sans-serif"><font size="2" style="font-size: 10pt">ttention mechanism, word representations, deep learning in nlp</font></font></span></font></font></p>
<p><font face="Poppins, Calibri, sans-serif" size="2"><b>Related Entries</b>: <a href="./context.html">Context</a>, <a href="./computational-linguistics.html">Computational Linguistics</a>, <a href="./discourse-(1).html">Discourse (1)</a>, <a href="./discourse-(2).html">Discourse (2)</a></font></p>
<p><font face="Poppins, Calibri, sans-serif" size="2"><b>References</b>:<br>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.‏<br>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.‏<br>Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). Improving language understanding by generative pre-training.‏</font></p>
</body>